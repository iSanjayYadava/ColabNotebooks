{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "004b_mixed_precision.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iSanjayYadava/ColabNotebooks/blob/master/004b_mixed_precision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "TrNgiCRM6RD1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xg0mhT4U6m8X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "9d1b8526-3acd-4f4b-d9dd-6d20886136aa"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZJLSt0SG6RD9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# FP16"
      ]
    },
    {
      "metadata": {
        "id": "zR4vPWZc6RD-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Intro"
      ]
    },
    {
      "metadata": {
        "id": "hg9KO9ol6REA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this notebook we are going to implement mixed precision floating points. \n",
        "\n",
        "By default, all computations are done in single-precision which means that all the floats (inputs, activations and weights) are 32-bit floats. If we could use 16-bit floats for each of these values, we would save half the space in RAM and this would enable us to double the size of our model and double the batch size (the first helping to get better results and the second to train quicker).\n",
        "\n",
        "However, half-precision floating points might lead to not-as-accurate results. Specifically, half-precision floating points can only represent 1, 1+2e-10, 1+2\\*2e-10 ... which in standard notation are 1, 1.0009765625, 1.001953125 ... (for more information on the limitations of the encoding of half-precision floating point numbers click [here](https://en.wikipedia.org/wiki/Half-precision_floating-point_format)). There are some specific calculations where this lack of acccuracy will impact our results. These are:\n",
        "\n",
        "1. When updating the weights we basically do _w=w-lr*w.grad_ for each weight and usually _lr*w.grad_ is several orders of magnitude below *w*. When this happens (e.g. _w=1_ and _lr*w.grad=0.0001_) the update will make no effect.\n",
        "2. Your gradients may be replaced by 0 because they are too low (underflow).\n",
        "3. Activations or loss may hit nan/infinity (overflow) and training might more easily diverge.\n",
        "\n",
        "To address these problems we will use a combination of different strategies.\n",
        "\n",
        "To take care of 1 and 3, we will use sigle-precision floating points for some parameters in the training. \n",
        "\n",
        "For 1, it’s okay if *w* and *grad* are both half floats, but when we do the operation _w = w - lr * grad_, we need to compute it in FP32. To achieve this, we will keep a copy of the weights in FP32 precision (from now on, master model) where we will update and then copy over to the original model. When we copy the weights into the original model we will lose precision, but the updated weight will be kept in FP32 in the master model so that, when the updates add up to a value that can be represented in FP16, the original model can tell the difference (i.e. if the update is +0.0001, the new weight value is updated it will be 1.0001 and the original model will not be able to tell the difference but if it is updated five times the new weight value will be 1.0005 and the original model will incorporate it as 1.0005).\n",
        "\n",
        "For 3, we will simply keep our batchnorms in single-precision (so our activations are in single precision) and our loss in single-precision (done by converting the last output of the model to single precision before passing it to the loss).\n",
        "\n",
        "For 2, we will take a different approach, called gradient scaling. We multiply the loss by a scale factor to place the values in a scale that FP16 can handle with more precision. We will then calculate the gradients by backpropagation and, before updating the weights, we will rescale the gradients to the original scale by dividing by _scale_ (remember that, because of the solution proposed in 1, we will update the weights in FP32 in the master model)."
      ]
    },
    {
      "metadata": {
        "id": "tRZYOBa96REB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Set up"
      ]
    },
    {
      "metadata": {
        "id": "6sdAwbUg6REC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#export\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/ColabNotebooks/')\n",
        "from nb_004a import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wGwCSHID6REG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DATA_PATH = Path('/content/gdrive/My Drive/ColabNotebooks/data')\n",
        "PATH = DATA_PATH/'cifar10_dog_air'\n",
        "#PATH = DATA_PATH/'cifar10'\n",
        "\n",
        "data_mean,data_std = map(tensor, ([0.491, 0.482, 0.447], [0.247, 0.243, 0.261]))\n",
        "cifar_norm,cifar_denorm = normalize_funcs(data_mean, data_std)\n",
        "\n",
        "train_tfms = [flip_lr(p=0.5),\n",
        "              pad(padding=4),\n",
        "              crop(size=32, row_pct=(0,1.), col_pct=(0,1.))]\n",
        "valid_tfms = []\n",
        "\n",
        "bs = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "odM5qFvC6REK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#export\n",
        "def to_half(b:Collection[Tensor])->Collection[Tensor]:  \n",
        "    \"[x,y] -> [x.half(),y] (half precision)\"\n",
        "    return [b[0].half(), b[1]]\n",
        "\n",
        "def compose(*funcs:Callable)->Callable:\n",
        "    \"Compose list of funcs\"\n",
        "    def compose_(funcs, x, *args, **kwargs):\n",
        "        for f in listify(funcs): x = f(x, *args, **kwargs)\n",
        "        return x\n",
        "    return partial(compose_, funcs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CD6CJQkr6REO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "76593291-56d6-4824-d719-a134453b9ded"
      },
      "cell_type": "code",
      "source": [
        "train_ds = ImageDataset.from_folder(PATH/'train', classes=['airplane','dog'])\n",
        "valid_ds = ImageDataset.from_folder(PATH/'test', classes=['airplane','dog'])\n",
        "data = DataBunch.create(train_ds, valid_ds, bs=bs, num_workers=0, \n",
        "                        train_tfm=train_tfms, valid_tfm=valid_tfms, dl_tfms=cifar_norm)\n",
        "len(data.train_dl), len(data.valid_dl)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(129, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "bBf7QTHf6RET",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "metrics = [accuracy]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "92UPS-Qj6REX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Darknet([1, 2, 2, 2, 2], num_classes=2, nf=16)\n",
        "learn = Learner(data, model, metrics=metrics)\n",
        "sched = OneCycleScheduler(learn, 0.1, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jd1AHHO86REa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# FP16"
      ]
    },
    {
      "metadata": {
        "id": "Hj1H8gGv6REb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ]
    },
    {
      "metadata": {
        "id": "meL5oPUZ6REc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#export\n",
        "def bn2float(module:nn.Module)->nn.Module:\n",
        "    \"If a module is batchnorm don't use half precision\"\n",
        "    if isinstance(module, torch.nn.modules.batchnorm._BatchNorm): module.float()\n",
        "    for child in module.children(): bn2float(child)\n",
        "    return module\n",
        "\n",
        "def model2half(model:nn.Module)->nn.Module:\n",
        "    \"Converts the model to half precision except the batchnorm layers\"\n",
        "    return bn2float(model.half())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tQEHk1cg6REg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Helper function to save the master model in FP32 with flat tensors (apparently it helps with performance)"
      ]
    },
    {
      "metadata": {
        "id": "cwtaCwlF6REh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#export\n",
        "from torch._utils import _unflatten_dense_tensors\n",
        "from torch.nn.utils import parameters_to_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Yp8owXJ6REk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we will implement the three changes we noted above. A summary of the steps we will follow is:\n",
        "\n",
        "\n",
        "    1. Compute the output with the FP16 model, then the loss\n",
        "    2. Back-propagate the gradients in half-precision\n",
        "    3. Copy the gradients in FP32 precision\n",
        "    4. Do the update on the master model (in FP32 precision)\n",
        "    5. Copy the master model in the FP16 model\n"
      ]
    },
    {
      "metadata": {
        "id": "n0J7HRE76REl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#export\n",
        "def get_master(layer_groups:ModuleList, flat_master:bool=False) -> Tuple[List[List[Tensor]], List[List[Tensor]]]:\n",
        "    \"Returns two lists, one for the model parameters in FP16 and one for the master parameters in FP32\"\n",
        "    split_groups = split_bn_bias(layer_groups)\n",
        "    model_params = [[param for param in lg.parameters() if param.requires_grad] for lg in split_groups]\n",
        "    if flat_master:\n",
        "        master_params = []\n",
        "        for lg in model_params:\n",
        "            if len(lg) !=0 :\n",
        "                mp = parameters_to_vector([param.data.float() for param in lg])\n",
        "                mp = torch.nn.Parameter(mp, requires_grad=True)\n",
        "                if mp.grad is None: mp.grad = mp.new(*mp.size())\n",
        "                master_params.append([mp])\n",
        "            else: master_params.append([])\n",
        "        return model_params, master_params\n",
        "    else:\n",
        "        master_params = [[param.clone().float().detach() for param in lg] for lg in model_params]\n",
        "        for mp in master_params:\n",
        "            for param in mp: param.requires_grad = True\n",
        "        return model_params, master_params\n",
        "\n",
        "def model_g2master_g(model_params:Sequence[Tensor], master_params:Sequence[Tensor], flat_master:bool=False)->None:\n",
        "    \"Copies the model gradients to the master parameters for the optimizer step\"\n",
        "    if flat_master:\n",
        "        for model_group,master_group in zip(model_params,master_params):\n",
        "            if len(master_group) != 0:\n",
        "                master_group[0].grad.data.copy_(parameters_to_vector([p.grad.data.float() for p in model_group]))\n",
        "    else:\n",
        "        for model_group,master_group in zip(model_params,master_params):\n",
        "            for model, master in zip(model_group, master_group):\n",
        "                if model.grad is not None:\n",
        "                    if master.grad is None: master.grad = master.data.new(*master.data.size())\n",
        "                    master.grad.data.copy_(model.grad.data)\n",
        "                else: master.grad = None\n",
        "\n",
        "def master2model(model_params:Sequence[Tensor], master_params:Sequence[Tensor], flat_master:bool=False)->None:\n",
        "    \"Copy master parameters to model parameters\"\n",
        "    if flat_master:\n",
        "        for model_group,master_group in zip(model_params,master_params):\n",
        "            if len(model_group) != 0:\n",
        "                for model, master in zip(model_group, _unflatten_dense_tensors(master_group[0].data, model_group)):\n",
        "                    model.data.copy_(master)\n",
        "    else:\n",
        "        for model_group,master_group in zip(model_params,master_params):\n",
        "            for model, master in zip(model_group, master_group): model.data.copy_(master.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5cCHMD0e6REn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## MixedPrecision"
      ]
    },
    {
      "metadata": {
        "id": "M_F4Z4vp6REo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#export\n",
        "from torch._utils import _unflatten_dense_tensors\n",
        "from torch.nn.utils import parameters_to_vector\n",
        "\n",
        "@dataclass\n",
        "class MixedPrecision(Callback):\n",
        "    \"Callback that handles mixed-precision training\"\n",
        "    learn:Learner\n",
        "    loss_scale:float=512.\n",
        "    flat_master:bool=False\n",
        "    def __post_init__(self): assert torch.backends.cudnn.enabled, \"Mixed precision training requires cudnn.\" \n",
        "    \n",
        "    def on_train_begin(self, **kwargs:Any)->None:\n",
        "        \"Ensures everything is in half precision mode\"\n",
        "#         self.learn.data.train_dl.half = True\n",
        "        self.learn.data.train_dl.add_tfm(to_half)\n",
        "        if hasattr(self.learn.data, 'valid_dl') and self.learn.data.valid_dl is not None:\n",
        "#             self.learn.data.valid_dl.half = True\n",
        "            self.learn.data.valid_dl.add_tfm(to_half)\n",
        "        #Get a copy of the model params in FP32\n",
        "        self.model_params, self.master_params = get_master(self.learn.layer_groups, self.flat_master)\n",
        "        #Changes the optimizer so that the optimization step is done in FP32.\n",
        "        opt = self.learn.opt\n",
        "        mom,wd,beta = opt.mom,opt.wd,opt.beta\n",
        "        lrs = [lr for lr in self.learn.opt._lr for _ in range(2)]\n",
        "        opt_params = [{'params': mp, 'lr': lr} for mp,lr in zip(self.master_params, lrs)]\n",
        "        self.learn.opt.opt = self.learn.opt_fn(opt_params)\n",
        "        opt.mom,opt.wd,opt.beta = mom,wd,beta\n",
        "        \n",
        "    def on_train_end(self, **kwargs:Any)->None:\n",
        "        \"Removes half precision transforms added at `on_train_begin`\"\n",
        "        self.learn.data.train_dl.remove_tfm(to_half)\n",
        "        if hasattr(self.learn.data, 'valid_dl') and self.learn.data.valid_dl is not None:\n",
        "            self.learn.data.valid_dl.remove_tfm(to_half)\n",
        "    \n",
        "    def on_loss_begin(self, last_output:Tensor, **kwargs:Any) -> Tensor:\n",
        "        \"Converts half precision output to FP32 to avoid reduction overflow.\"\n",
        "        return last_output.float()\n",
        "    \n",
        "    def on_backward_begin(self, last_loss:Rank0Tensor, **kwargs:Any) -> Rank0Tensor:\n",
        "        \"Scale gradients up by `loss_scale` to prevent underflow\"\n",
        "        #To avoid gradient underflow, we scale the gradients\n",
        "        return last_loss * self.loss_scale\n",
        "    \n",
        "    def on_backward_end(self, **kwargs:Any ):\n",
        "        \"Convert the gradients back to FP32 and divide them by the scale.\"\n",
        "        model_g2master_g(self.model_params, self.master_params, self.flat_master)\n",
        "        for group in self.master_params:\n",
        "            for param in group: param.grad.div_(self.loss_scale)\n",
        "    \n",
        "    def on_step_end(self, **kwargs:Any)->None:\n",
        "        \"Update the params from master to model and zero grad\"\n",
        "        #Zeros the gradients of the model since the optimizer is disconnected.\n",
        "        self.learn.model.zero_grad()\n",
        "        #Update the params from master to model.\n",
        "        master2model(self.model_params, self.master_params, self.flat_master)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yrt3JRr96REq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def mixed_precision(loss_scale:float=512., flat_master:bool=False, **kwargs:Any)->MixedPrecision:\n",
        "    return partial(MixedPrecision, loss_scale=loss_scale, flat_master=flat_master, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CPZdYfQD6REt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cbs = [one_cycle_scheduler(0.1)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RLixPm0o6REv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Darknet([1, 2, 2, 2, 2], num_classes=2, nf=16)\n",
        "model = model2half(model)\n",
        "learn = Learner(data, model, metrics=accuracy, callback_fns=cbs)\n",
        "mp_cb = MixedPrecision(learn, flat_master=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QcTp-mIP6REz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.fit(1, 1e-2, callbacks=mp_cb)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rpxbvIL86RE2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "024552c7-e58e-4e23-fab8-23dac1241413"
      },
      "cell_type": "code",
      "source": [
        "learn.model.layers[0][0].weight.type()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'torch.HalfTensor'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "fOr_Oyu06RE7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "76a9093a-3d66-4a6c-ef92-b0393c85bab4"
      },
      "cell_type": "code",
      "source": [
        "mp_cb.master_params[0][0].size(),mp_cb.master_params[0][0].type()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([5059506]), 'torch.FloatTensor')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "VCAo0G3l6RE-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## to_fp16"
      ]
    },
    {
      "metadata": {
        "id": "BBiasZKY6RE-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#export\n",
        "def to_fp16(learn:Learner, loss_scale:float=512., flat_master:bool=False)->Learner:\n",
        "    \"Transforms the learner in FP16 precision\"\n",
        "    learn.model = model2half(learn.model)\n",
        "    learn.mp_cb = MixedPrecision(learn, loss_scale=loss_scale, flat_master=flat_master)\n",
        "    learn.callbacks.append(learn.mp_cb)\n",
        "    return learn\n",
        "\n",
        "Learner.to_fp16 = to_fp16"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "begYS2cY6RFB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7534
        },
        "outputId": "9adf6134-d4eb-42c6-a366-29724d06e872"
      },
      "cell_type": "code",
      "source": [
        "model = Darknet([1, 2, 2, 2, 2], num_classes=2, nf=16)\n",
        "learn = Learner(data, model, metrics=accuracy, callback_fns=cbs)\n",
        "learn.to_fp16(flat_master=True)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Learner(data=<nb_002b.DataBunch object at 0x7f3ff9ce5898>, model=Darknet(\n",
              "  (layers): Sequential(\n",
              "    (0): Sequential(\n",
              "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "    )\n",
              "    (1): Sequential(\n",
              "      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "    )\n",
              "    (2): ResLayer(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "      )\n",
              "    )\n",
              "    (3): Sequential(\n",
              "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "    )\n",
              "    (4): ResLayer(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "      )\n",
              "    )\n",
              "    (5): ResLayer(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "      )\n",
              "    )\n",
              "    (6): Sequential(\n",
              "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "    )\n",
              "    (7): ResLayer(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "      )\n",
              "    )\n",
              "    (8): ResLayer(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "      )\n",
              "    )\n",
              "    (9): Sequential(\n",
              "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "    )\n",
              "    (10): ResLayer(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "      )\n",
              "    )\n",
              "    (11): ResLayer(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "      )\n",
              "    )\n",
              "    (12): Sequential(\n",
              "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "    )\n",
              "    (13): ResLayer(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "      )\n",
              "    )\n",
              "    (14): ResLayer(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "      )\n",
              "    )\n",
              "    (15): AdaptiveAvgPool2d(output_size=1)\n",
              "    (16): Lambda()\n",
              "    (17): Linear(in_features=512, out_features=2, bias=True)\n",
              "  )\n",
              "), opt_fn=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_fn=<function cross_entropy at 0x7f3ff9ef82f0>, metrics=[<function accuracy at 0x7f3ff9cc0510>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='models', callback_fns=[<class 'nb_004.Recorder'>, functools.partial(<class 'nb_004.OneCycleScheduler'>, lr_max=0.1)], callbacks=[MixedPrecision(learn=Learner(data=<nb_002b.DataBunch object at 0x7f3ff9ce5898>, model=Darknet(\n",
              "  (layers): Sequential(\n",
              "    (0): Sequential(\n",
              "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "    )\n",
              "    (1): Sequential(\n",
              "      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "    )\n",
              "    (2): ResLayer(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "      )\n",
              "    )\n",
              "    (3): Sequential(\n",
              "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "    )\n",
              "    (4): ResLayer(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "      )\n",
              "    )\n",
              "    (5): ResLayer(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "      )\n",
              "    )\n",
              "    (6): Sequential(\n",
              "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "    )\n",
              "    (7): ResLayer(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "      )\n",
              "    )\n",
              "    (8): ResLayer(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "      )\n",
              "    )\n",
              "    (9): Sequential(\n",
              "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "    )\n",
              "    (10): ResLayer(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "      )\n",
              "    )\n",
              "    (11): ResLayer(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "      )\n",
              "    )\n",
              "    (12): Sequential(\n",
              "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "    )\n",
              "    (13): ResLayer(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "      )\n",
              "    )\n",
              "    (14): ResLayer(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "      )\n",
              "    )\n",
              "    (15): AdaptiveAvgPool2d(output_size=1)\n",
              "    (16): Lambda()\n",
              "    (17): Linear(in_features=512, out_features=2, bias=True)\n",
              "  )\n",
              "), opt_fn=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_fn=<function cross_entropy at 0x7f3ff9ef82f0>, metrics=[<function accuracy at 0x7f3ff9cc0510>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='models', callback_fns=[<class 'nb_004.Recorder'>, functools.partial(<class 'nb_004.OneCycleScheduler'>, lr_max=0.1)], callbacks=[...], layer_groups=[Sequential(\n",
              "  (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "  (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (5): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (6): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "  (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (8): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (9): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (11): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (12): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (14): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (15): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "  (16): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (17): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (18): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (20): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (21): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "  (22): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (23): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (24): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (25): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (26): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (27): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "  (28): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (29): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (30): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "  (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (32): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (33): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (34): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (35): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (36): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "  (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (38): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (39): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (40): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (41): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (42): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "  (43): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (44): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (45): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "  (46): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (47): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (48): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (49): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (50): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (51): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "  (52): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (53): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (54): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (55): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (56): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (57): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "  (58): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (59): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (60): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "  (61): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (62): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (63): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (64): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (65): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (66): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "  (67): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (68): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (69): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (70): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (71): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (72): AdaptiveAvgPool2d(output_size=1)\n",
              "  (73): Lambda()\n",
              "  (74): Linear(in_features=512, out_features=2, bias=True)\n",
              ")]), loss_scale=512.0, flat_master=True)], layer_groups=[Sequential(\n",
              "  (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (2): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "  (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (5): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (6): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "  (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (8): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (9): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (11): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (12): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (14): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (15): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "  (16): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (17): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (18): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (20): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (21): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "  (22): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (23): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (24): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (25): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (26): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (27): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "  (28): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (29): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (30): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "  (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (32): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (33): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (34): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (35): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (36): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "  (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (38): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (39): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (40): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (41): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (42): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "  (43): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (44): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (45): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "  (46): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (47): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (48): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (49): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (50): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (51): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "  (52): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (53): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (54): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (55): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (56): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (57): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "  (58): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (59): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (60): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "  (61): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (62): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (63): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (64): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (65): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (66): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "  (67): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (68): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (69): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (70): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (71): LeakyReLU(negative_slope=0.1, inplace)\n",
              "  (72): AdaptiveAvgPool2d(output_size=1)\n",
              "  (73): Lambda()\n",
              "  (74): Linear(in_features=512, out_features=2, bias=True)\n",
              ")])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "A1J0YgTk6RFD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.fit(1, 1e-2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tjkW8S2l6RFG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7dc414f0-4780-4150-977e-9ee1318db04c"
      },
      "cell_type": "code",
      "source": [
        "learn.mp_cb.master_params[0][0].size()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5059506])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "evRaLM2a6RFI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Test with discriminative lrs"
      ]
    },
    {
      "metadata": {
        "id": "c0CMi-6R6RFJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Darknet([1, 2, 2, 2, 2], num_classes=2, nf=16)\n",
        "model = model2half(model)\n",
        "learn = Learner(data, model, metrics=accuracy)\n",
        "\n",
        "learn.split(lambda m: split_model(m,[m.layers[9],m.layers[15]]))\n",
        "cbs = [MixedPrecision(learn, flat_master=True), OneCycleScheduler(learn, 0.1)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KABOt8VR6RFL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.fit(1, 1e-2, callbacks=cbs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W42NMMpm6RFP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d0c0436d-136e-42df-e759-15ef0110c8bd"
      },
      "cell_type": "code",
      "source": [
        "learn.model.layers[0][0].weight.type()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'torch.HalfTensor'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "EOAaQ9WU6RFR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for master in cbs[0].master_params:\n",
        "    print(master[0].size(),master[0].type())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bZnSNXYY6RFV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}